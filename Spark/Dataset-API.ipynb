{"cells":[{"cell_type":"markdown","source":["# Dataset API\n\nIn this notebook, we demonstrate the new Dataset API in Spark 2.0, using a very simple JSON file."],"metadata":{}},{"cell_type":"code","source":["%scala\ncase class Person (name: String, age: Long)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">defined class Person\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["%scala\nval ds = spark.read.json(\"/databricks-datasets/samples/people/people.json\").as[Person]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ds: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### Metadata operations\n\nThere are a few metadata operations that are very handy for Datasets."],"metadata":{}},{"cell_type":"code","source":["%scala\n// Get the list of columns\nds.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: Array[String] = Array(age, name)\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["%scala\n// Get the schema of the underlying data structure.\nds.schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: org.apache.spark.sql.types.StructType = StructType(StructField(age,LongType,true), StructField(name,StringType,true))\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["%scala\n// Explain the logical and physical query plan to compute the Dataset.\nds.explain(true)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Parsed Logical Plan ==\nRelation[age#180L,name#181] json\n\n== Analyzed Logical Plan ==\nage: bigint, name: string\nRelation[age#180L,name#181] json\n\n== Optimized Logical Plan ==\nRelation[age#180L,name#181] json\n\n== Physical Plan ==\n*(1) FileScan json [age#180L,name#181] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[dbfs:/databricks-datasets/samples/people/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;age:bigint,name:string&gt;\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Typed Dataset API\n\nDataset includes a typed functional API similar to RDDs and Scala's own collection library. This API is available in Scala/Java, but not Python/R."],"metadata":{}},{"cell_type":"code","source":["%scala\n// Run a map\nds.map(_.name).collect()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res4: Array[String] = Array(Jane, Andy, Justin)\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["%scala \nval ds1 = ds.filter(x=> x.age > 20)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ds1: org.apache.spark.sql.Dataset[Person] = [age: bigint, name: string]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["%scala\ndisplay(ds1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th></tr></thead><tbody><tr><td>40</td><td>Jane</td></tr><tr><td>30</td><td>Andy</td></tr><tr><td>50</td><td>Justin</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"code","source":["%scala\nval ds2 = ds1.map(x => x.age)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ds2: org.apache.spark.sql.Dataset[Long] = [value: bigint]\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["%scala\ndisplay(ds2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>40</td></tr><tr><td>30</td></tr><tr><td>50</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"code","source":["%scala\nds.map(_.name).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res9: Array[String] = Array(Jane, Andy, Justin)\n</div>"]}}],"execution_count":14}],"metadata":{"name":"Dataset-API","notebookId":2266549970539408},"nbformat":4,"nbformat_minor":0}
